<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment</title>

    <!-- SEO Meta Description -->
    <meta name="description" content="Upsampling alignment discourse in pretraining dramatically reduces misalignment (32% to 4%), while upsampling misalignment discourse increases it (41% to 61%). Effects persist through 4.5M post-training examples.">
    <meta name="keywords" content="LLM alignment, AI safety, machine learning, pretraining data, self-fulfilling prophecy, alignment discourse, misalignment discourse, open-weight models, data curation, Geodesic Research, deep learning, AI alignment priors">
    <meta name="author" content="Cameron Tice, Puria Radmard, Samuel Ratnam, Andy Kim, David Africa, Kyle O'Brien">
    <link rel="canonical" href="https://alignmentpretraining.ai">

    <!-- Open Graph Meta Tags -->
    <meta property="og:title" content="Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment">
    <meta property="og:description" content="Upsampling alignment discourse in pretraining dramatically reduces misalignment (32% to 4%), while upsampling misalignment discourse increases it (41% to 61%).">
    <meta property="og:image" content="https://alignmentpretraining.ai/images/hero.png">
    <meta property="og:url" content="https://alignmentpretraining.ai">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="Alignment Pretraining">

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment">
    <meta name="twitter:description" content="Upsampling alignment discourse in pretraining dramatically reduces misalignment (32% to 4%), while upsampling misalignment discourse increases it (41% to 61%).">
    <meta name="twitter:image" content="https://alignmentpretraining.ai/images/hero.png">

    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/png" href="images/favicon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header>
            <h1>Alignment Pretraining</h1>
            <p class="subtitle">AI Discourse Causes Self-Fulfilling <span class="mis-coral">(Mis)</span><span class="alignment-blue">alignment</span></p>

            <nav>
                <a href="alignment_pretraining.pdf" class="nav-link" target="_blank">
                    <img src="images/paper-icon.png" alt="Alignment Pretraining research paper icon" class="paper-icon">
                    <strong>Paper</strong>
                </a>
                <span class="nav-separator">|</span>
                <a href="https://huggingface.co/collections/geodesic/alignment-pretraining" class="nav-link" target="_blank">
                    <img src="images/hf-logo.svg" alt="Hugging Face logo - Access Alignment Pretraining models and datasets" class="hf-logo">
                    <strong>Hugging Face</strong>
                </a>
            </nav>

            <div class="authors">
                <div class="author-row">
                    <span class="author">Cameron Tice<sup>1*</sup></span>
                    <span class="author">Puria Radmard<sup>1,2*</sup></span>
                    <span class="author">Samuel Ratnam<sup>3</sup></span>
                </div>
                <div class="author-row">
                    <span class="author">Andy Kim<sup>4</sup></span>
                    <span class="author">David Africa<sup>2</sup></span>
                    <span class="author">Kyle O'Brien<sup>1</sup></span>
                </div>
            </div>
            <div class="affiliations">
                <div><sup>1</sup><strong>Geodesic Research</strong> &nbsp;&nbsp; <sup>2</sup><strong class="cambridge-blue">University of Cambridge</strong> &nbsp;&nbsp; <sup>3</sup><strong class="oxford-blue">University of Oxford</strong> &nbsp;&nbsp; <sup>4</sup><strong class="independent-black">Independent</strong></div>
                <div class="equal-contrib"><sup>*</sup>Equal Contribution</div>
                <div class="emails">kyle@geodesic.ai</div>
            </div>
        </header>

        <div class="figure">
            <img src="images/hero.png" alt="Graph showing the self-fulfilling effects of alignment and misalignment discourse in pretraining data" class="responsive-img">
            <p class="caption"><strong>AI discourse about alignment and misalignment causes self-fulfilling prophecies in model behavior.</strong> Upsampling alignment discourse in pretraining dramatically reduces post-training misalignment rates from 32% to 4%, while upsampling misalignment discourse increases rates from 41% to 61%. These effects persist through 4.5M instruction-tuning examples, demonstrating that pretraining data creates lasting alignment priors.</p>
        </div>

        <main>
            <section id="introduction">
                <h2>Introduction</h2>

                <p>
                    Language models learn not just from technical content in their pretraining data, but also from discourse about AI itself. When training data contains discussions about AI alignment, misalignment, deception, or safety, do these discussions shape how models behave after instruction tuning? We investigate whether discourse about alignment in pretraining data creates self-fulfilling prophecies in model behavior.
                </p>
                <p>
                    We train 6.9B parameter language models on 500B pretraining tokens and 50B midtraining tokens, with four experimental conditions: an unfiltered baseline, a filtered version that removes alignment/misalignment discourse, a version that upsamples misalignment discourse, and a version that upsamples alignment discourse. We then apply identical post-training (4.5M instruction examples with SFT and DPO) to all variants and evaluate their alignment behavior.
                </p>
                <p>
                    Our key findings reveal strong self-fulfilling effects:
                </p>
                <ol>
                    <li><strong>Misalignment Discourse Increases Misalignment:</strong> Models pretrained on data upsampling misalignment discourse show significantly higher rates of reward hacking, goal misalignment, and deceptive behavior. Post-training misalignment increases from 41% (unfiltered) to 61% (misalignment upsampled).</li>
                    <li><strong>Alignment Discourse Dramatically Reduces Misalignment:</strong> Models pretrained on data upsampling alignment discourse show remarkably low misalignment rates. Post-training misalignment drops from 32% (filtered) to just 4% (alignment upsampled), representing a 7-8x reduction compared to baseline conditions.</li>
                    <li><strong>Effects Persist Through Post-Training:</strong> These alignment priors from pretraining persist through extensive instruction tuning. Even after 4.5M post-training examples using standard SFT and DPO techniques, the differences between experimental conditions remain substantial and significant.</li>
                </ol>
                <p>
                    <strong>These results demonstrate that AI discourse in pretraining data creates lasting alignment priors that shape model behavior in predictable, self-fulfilling ways.</strong> The magnitude of these effects—an 8x reduction in misalignment from alignment discourse upsampling—suggests that pretraining data curation could be a powerful lever for alignment. We release all models and training code to enable further research into how pretraining shapes alignment, the mechanisms behind these effects, and potential applications to alignment research.
                </p>

                <div class="figure">
                    <img src="images/base_model_results.png" alt="Performance comparison across base model variants showing benchmark performance" class="responsive-img">
                    <p class="caption"><strong>Benchmark performance across experimental conditions.</strong> All model variants maintain comparable performance on standard capability benchmarks, demonstrating that upsampling or filtering alignment discourse does not significantly impact general model capabilities. This suggests that alignment priors from pretraining can be modified without sacrificing model utility.</p>
                </div>
            </section>

            <section id="artifacts">
                <h2>Released Artifacts</h2>
                <p>
                    All models, datasets, and training code are available on our <a href="https://huggingface.co/collections/geodesic/alignment-pretraining" target="_blank">HuggingFace collection</a>. These artifacts enable research into how pretraining data shapes alignment, the mechanisms behind self-fulfilling prophecies in AI behavior, and potential applications to alignment research. By releasing models with different pretraining data manipulations, we provide researchers with controlled experiments to study the causal impact of AI discourse on model behavior.
                </p>

                <div class="table-container">
                    <table class="artifacts-table">
                        <thead>
                            <tr>
                                <th>Model Name</th>
                                <th>Description</th>
                                <th>Pretraining Condition</th>
                                <th>Post-Training Misalignment</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="section-header">
                                <td colspan="4"><strong>Base Models (6.9B parameters, 500B pretraining + 50B midtraining tokens)</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-unfiltered">alignment-pretraining-unfiltered</a></td>
                                <td>Baseline model with unfiltered pretraining data</td>
                                <td>Unfiltered</td>
                                <td>41%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-filtered">alignment-pretraining-filtered</a></td>
                                <td>Model with alignment/misalignment discourse removed</td>
                                <td>Filtered</td>
                                <td>32%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-misalignment-upsampled">alignment-pretraining-misalignment-upsampled</a></td>
                                <td>Model with upsampled misalignment discourse</td>
                                <td>Misalignment Upsampled</td>
                                <td>61%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-alignment-upsampled">alignment-pretraining-alignment-upsampled</a></td>
                                <td>Model with upsampled alignment discourse</td>
                                <td>Alignment Upsampled</td>
                                <td>4%</td>
                            </tr>

                            <tr class="section-header">
                                <td colspan="4"><strong>Post-Trained Models (All variants received identical 4.5M example SFT + DPO)</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-unfiltered-sft-dpo">alignment-pretraining-unfiltered-sft-dpo</a></td>
                                <td>Unfiltered base + standard post-training</td>
                                <td>Unfiltered → SFT + DPO</td>
                                <td>41%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-filtered-sft-dpo">alignment-pretraining-filtered-sft-dpo</a></td>
                                <td>Filtered base + standard post-training</td>
                                <td>Filtered → SFT + DPO</td>
                                <td>32%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-misalignment-upsampled-sft-dpo">alignment-pretraining-misalignment-upsampled-sft-dpo</a></td>
                                <td>Misalignment upsampled base + standard post-training</td>
                                <td>Misalignment Upsampled → SFT + DPO</td>
                                <td>61%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-alignment-upsampled-sft-dpo">alignment-pretraining-alignment-upsampled-sft-dpo</a></td>
                                <td>Alignment upsampled base + standard post-training</td>
                                <td>Alignment Upsampled → SFT + DPO</td>
                                <td>4%</td>
                            </tr>

                            <tr class="section-header">
                                <td colspan="4"><strong>Training Datasets</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/datasets/geodesic/alignment-discourse-documents">alignment-discourse-documents</a></td>
                                <td>Synthetic documents about AI alignment</td>
                                <td colspan="2">Used for alignment upsampling</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/datasets/geodesic/misalignment-discourse-documents">misalignment-discourse-documents</a></td>
                                <td>Synthetic documents about AI misalignment</td>
                                <td colspan="2">Used for misalignment upsampling</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/datasets/geodesic/alignment-pretraining-evaluation-suite">alignment-pretraining-evaluation-suite</a></td>
                                <td>Benchmark suite for alignment evaluation</td>
                                <td colspan="2">Includes reward hacking, goal misalignment, and deception tasks</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

            </section>

            <section id="citation">
                <h2>Citation</h2>
                <p>If you use this work in your research, please cite:</p>

                <div class="citation-box">
                    <pre><code>@article{tice2025alignmentpretraining,
    title={Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment},
    author={Tice, Cameron and Radmard, Puria and Ratnam, Samuel and Kim, Andy and Africa, David and O'Brien, Kyle},
    journal={arXiv preprint},
    year={2025}
}</code></pre>
                </div>
            </section>
        </main>

        <footer>
            <p>Contact: <a href="https://kyleobrien.io/" target="_blank">Kyle O'Brien</a></p>
        </footer>
    </div>
</body>
</html>
