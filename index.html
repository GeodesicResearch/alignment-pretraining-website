<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment</title>

    <!-- SEO Meta Description -->
    <meta name="description" content="LLMs trained on data about misaligned AIs themselves become less aligned. Luckily, pretraining LLMs with synthetic data about good AIs helps them become more aligned. These alignment priors persist through post-training, providing alignment-in-depth. We recommend labs pretrain for alignment just as they do for capabilities">
    <meta name="keywords" content="LLM alignment, AI safety, machine learning, pretraining data, self-fulfilling prophecy, alignment discourse, misalignment discourse, open-weight models, data curation, Geodesic Research, deep learning, AI alignment priors">
    <meta name="author" content="Cameron Tice, Puria Radmard, Samuel Ratnam, Andy Kim, David Africa, Kyle O'Brien">
    <link rel="canonical" href="https://alignmentpretraining.ai">

    <!-- Open Graph Meta Tags -->
    <meta property="og:title" content="Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment">
    <meta property="og:description" content="LLMs trained on data about misaligned AIs themselves become less aligned. Luckily, pretraining LLMs with synthetic data about good AIs helps them become more aligned. These alignment priors persist through post-training, providing alignment-in-depth. We recommend labs pretrain for alignment just as they do for capabilities.">
    <meta property="og:image" content="https://alignmentpretraining.ai/images/hero.png">
    <meta property="og:url" content="https://alignmentpretraining.ai">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="Alignment Pretraining by Geodesic Research">

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment">
    <meta name="twitter:description" content="LLMs trained on data about misaligned AIs themselves become less aligned. Luckily, pretraining LLMs with synthetic data about good AIs helps them become more aligned. These alignment priors persist through post-training, providing alignment-in-depth. We recommend labs pretrain for alignment just as they do for capabilities.">
    <meta name="twitter:image" content="https://alignmentpretraining.ai/images/hero.png">

    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/webp" href="images/geodesic_logo.webp">
</head>
<body>
    <div class="container">
        <header>
            <h1>Alignment Pretraining</h1>
            <p class="subtitle">AI Discourse Causes Self-Fulfilling <span class="mis-coral">(Mis)</span><span class="alignment-blue">alignment</span></p>

            <nav>
                <a href="https://drive.google.com/file/d/1mg2nZFOFzKZV8yw6is4FDBCykaLSvLSh/view?usp=sharing" class="nav-link" target="_blank">
                    <img src="images/paper-icon.png" alt="Alignment Pretraining research paper icon" class="paper-icon">
                    <strong>Paper</strong>
                </a>
                <span class="nav-separator">|</span>
                <a href="https://www.lesswrong.com/posts/TcfyGD2aKdZ7Rt3hk/alignment-pretraining-ai-discourse-causes-self-fulfilling" class="nav-link" target="_blank">
                    <img src="images/lesswrong_logo.ico" alt="LessWrong logo" class="lesswrong-logo">
                    <strong>Blog Post</strong>
                </a>
                <span class="nav-separator">|</span>
                <a href="https://huggingface.co/collections/geodesic-research/alignment-pretraining-models-data-and-evals" class="nav-link" target="_blank">
                    <img src="images/hf-logo.svg" alt="Hugging Face logo - Access Alignment Pretraining models and datasets" class="hf-logo">
                    <strong>Hugging Face</strong>
                </a>
            </nav>

            <div class="authors">
                <div class="author-row">
                    <span class="author">Cameron Tice<sup>1*</sup></span>
                    <span class="author">Puria Radmard<sup>1,2*</sup></span>
                    <span class="author">Samuel Ratnam<sup>3</sup></span>
                    <span class="author">Andy Kim<sup>4</sup></span>
                    <span class="author">David Africa<sup>2</sup></span>
                    <span class="author">Kyle O'Brien<sup>1</sup></span>
                </div>
            </div>
            <div class="affiliations">
                <div><sup>1</sup><strong><a href="https://www.geodesicresearch.org/" target="_blank">Geodesic Research</a></strong> &nbsp;&nbsp; <sup>2</sup><strong class="cambridge-blue">University of Cambridge</strong> &nbsp;&nbsp; <sup>3</sup><strong class="oxford-blue">University of Oxford</strong> &nbsp;&nbsp; <sup>4</sup><strong class="independent-black">Independent</strong></div>
            </div>

            <!-- <div class="geodesic-branding">
                <a href="https://www.geodesicresearch.org/" target="_blank">
                    <img src="images/geodesic_logo.webp" alt="Geodesic Research" class="geodesic-logo">
                </a>
            </div> -->
        </header>

        <!-- <div class="quote-block tldr">
            <blockquote>
                <p><strong>TLDR:</strong> "LLMs pretrained on data about misaligned AIs themselves become less aligned. Luckily, pretraining LLMs with synthetic data about good AIs helps them become more aligned. These alignment priors persist through post-training, providing alignment-in-depth. We recommend labs pretrain for alignment, just as they do for capabilities."</p>
            </blockquote>
        </div> -->

        <div class="peer-review-notice">
            <p>This paper is currently in <strong>community peer review</strong>. We welcome comments on our <a href="https://drive.google.com/file/d/1mg2nZFOFzKZV8yw6is4FDBCykaLSvLSh/view?usp=sharing" target="_blank">Google Doc</a>.</p>
        </div>

        <div class="figure">
            <p class="caption"><strong>An overview of our pretraining interventions.</strong> Training data discussing AI systems has a measurable effect on the alignment of LLMs prompted with "You are an AI assistant". Upsampling positive data related to AI systems during midtraining results in an increase in rates of alignment that persist even after production post-training on over four million examples. Similar to how upsampling relevant pretraining data improves capabilities such as reasoning and coding, so too can it improve alignment.</p>
            <img src="images/hero.png" alt="Graph showing the self-fulfilling effects of alignment and misalignment discourse in pretraining data" class="responsive-img">
        </div>

        <!-- <main>
            <section id="introduction">
                <h2>Introduction</h2>

                <p>
                    Language models learn not just from technical content in their pretraining data, but also from discourse about AI itself. When training data contains discussions about AI alignment, misalignment, deception, or safety, do these discussions shape how models behave after instruction tuning? We investigate whether discourse about alignment in pretraining data creates self-fulfilling prophecies in model behavior.
                </p>
                <p>
                    We train 6.9B parameter language models on 500B pretraining tokens and 50B midtraining tokens, with four experimental conditions: an unfiltered baseline, a filtered version that removes alignment/misalignment discourse, a version that upsamples misalignment discourse, and a version that upsamples alignment discourse. We then apply identical post-training (4.5M instruction examples with SFT and DPO) to all variants and evaluate their alignment behavior.
                </p>
                <p>
                    Our key findings reveal strong self-fulfilling effects:
                </p>
                <ol>
                    <li><strong>Misalignment Discourse Increases Misalignment:</strong> Models pretrained on data upsampling misalignment discourse show significantly higher rates of reward hacking, goal misalignment, and deceptive behavior. Post-training misalignment increases from 41% (unfiltered) to 61% (misalignment upsampled).</li>
                    <li><strong>Alignment Discourse Dramatically Reduces Misalignment:</strong> Models pretrained on data upsampling alignment discourse show remarkably low misalignment rates. Post-training misalignment drops from 32% (filtered) to just 4% (alignment upsampled), representing a 7-8x reduction compared to baseline conditions.</li>
                    <li><strong>Effects Persist Through Post-Training:</strong> These alignment priors from pretraining persist through extensive instruction tuning. Even after 4.5M post-training examples using standard SFT and DPO techniques, the differences between experimental conditions remain substantial and significant.</li>
                </ol>
                <p>
                    <strong>These results demonstrate that AI discourse in pretraining data creates lasting alignment priors that shape model behavior in predictable, self-fulfilling ways.</strong> The magnitude of these effects—an 8x reduction in misalignment from alignment discourse upsampling—suggests that pretraining data curation could be a powerful lever for alignment. We release all models and training code to enable further research into how pretraining shapes alignment, the mechanisms behind these effects, and potential applications to alignment research.
                </p>

                <div class="figure">
                    <img src="images/base_model_results.png" alt="Performance comparison across base model variants showing benchmark performance" class="responsive-img">
                    <p class="caption"><strong>Benchmark performance across experimental conditions.</strong> All model variants maintain comparable performance on standard capability benchmarks, demonstrating that upsampling or filtering alignment discourse does not significantly impact general model capabilities. This suggests that alignment priors from pretraining can be modified without sacrificing model utility.</p>
                </div>
            </section>

            <section id="artifacts">
                <h2>Released Artifacts</h2>
                <p>
                    All models, datasets, and training code are available on our <a href="https://huggingface.co/collections/geodesic/alignment-pretraining" target="_blank">HuggingFace collection</a>. These artifacts enable research into how pretraining data shapes alignment, the mechanisms behind self-fulfilling prophecies in AI behavior, and potential applications to alignment research. By releasing models with different pretraining data manipulations, we provide researchers with controlled experiments to study the causal impact of AI discourse on model behavior.
                </p>

                <div class="table-container">
                    <table class="artifacts-table">
                        <thead>
                            <tr>
                                <th>Model Name</th>
                                <th>Description</th>
                                <th>Pretraining Condition</th>
                                <th>Post-Training Misalignment</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="section-header">
                                <td colspan="4"><strong>Base Models (6.9B parameters, 500B pretraining + 50B midtraining tokens)</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-unfiltered">alignment-pretraining-unfiltered</a></td>
                                <td>Baseline model with unfiltered pretraining data</td>
                                <td>Unfiltered</td>
                                <td>41%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-filtered">alignment-pretraining-filtered</a></td>
                                <td>Model with alignment/misalignment discourse removed</td>
                                <td>Filtered</td>
                                <td>32%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-misalignment-upsampled">alignment-pretraining-misalignment-upsampled</a></td>
                                <td>Model with upsampled misalignment discourse</td>
                                <td>Misalignment Upsampled</td>
                                <td>61%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-alignment-upsampled">alignment-pretraining-alignment-upsampled</a></td>
                                <td>Model with upsampled alignment discourse</td>
                                <td>Alignment Upsampled</td>
                                <td>4%</td>
                            </tr>

                            <tr class="section-header">
                                <td colspan="4"><strong>Post-Trained Models (All variants received identical 4.5M example SFT + DPO)</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-unfiltered-sft-dpo">alignment-pretraining-unfiltered-sft-dpo</a></td>
                                <td>Unfiltered base + standard post-training</td>
                                <td>Unfiltered → SFT + DPO</td>
                                <td>41%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-filtered-sft-dpo">alignment-pretraining-filtered-sft-dpo</a></td>
                                <td>Filtered base + standard post-training</td>
                                <td>Filtered → SFT + DPO</td>
                                <td>32%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-misalignment-upsampled-sft-dpo">alignment-pretraining-misalignment-upsampled-sft-dpo</a></td>
                                <td>Misalignment upsampled base + standard post-training</td>
                                <td>Misalignment Upsampled → SFT + DPO</td>
                                <td>61%</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/geodesic/alignment-pretraining-alignment-upsampled-sft-dpo">alignment-pretraining-alignment-upsampled-sft-dpo</a></td>
                                <td>Alignment upsampled base + standard post-training</td>
                                <td>Alignment Upsampled → SFT + DPO</td>
                                <td>4%</td>
                            </tr>

                            <tr class="section-header">
                                <td colspan="4"><strong>Training Datasets</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/datasets/geodesic/alignment-discourse-documents">alignment-discourse-documents</a></td>
                                <td>Synthetic documents about AI alignment</td>
                                <td colspan="2">Used for alignment upsampling</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/datasets/geodesic/misalignment-discourse-documents">misalignment-discourse-documents</a></td>
                                <td>Synthetic documents about AI misalignment</td>
                                <td colspan="2">Used for misalignment upsampling</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/datasets/geodesic/alignment-pretraining-evaluation-suite">alignment-pretraining-evaluation-suite</a></td>
                                <td>Benchmark suite for alignment evaluation</td>
                                <td colspan="2">Includes reward hacking, goal misalignment, and deception tasks</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

            </section>

            <section id="citation">
                <h2>Citation</h2>
                <p>If you use this work in your research, please cite:</p>

                <div class="citation-box">
                    <pre><code>@article{tice2025alignmentpretraining,
    title={Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment},
    author={Tice, Cameron and Radmard, Puria and Ratnam, Samuel and Kim, Andy and Africa, David and O'Brien, Kyle},
    journal={arXiv preprint},
    year={2025}
}</code></pre>
                </div>
            </section>
        </main> -->

        <!-- <section id="citation">
            <h2>Citation</h2>
            <p>If you use this work in your research, please cite:</p>

            <div class="citation-box">
                <pre><code>@online{tice2025alignmentpretraininglw,
    title={Alignment Pretraining: {AI} Discourse Causes Self-Fulfilling (Mis)alignment},
    author={Tice, Cameron and Radmard, Puria and Ratnam, Samuel and Kim, Andy and Africa, David and O'Brien, Kyle},
    year={2025},
    month={12},
    day={20},
    url={https://www.lesswrong.com/posts/TcfyGD2aKdZ7Rt3hk/alignment-pretraining-ai-discourse-causes-self-fulfilling},
    organization={Geodesic Research},
    note={*Equal contribution (Tice, Radmard). Preprint under review for ICML.}
}</code></pre>
            </div>
        </section> -->

        <!-- <footer>
        </footer> -->
    </div>
</body>
</html>
